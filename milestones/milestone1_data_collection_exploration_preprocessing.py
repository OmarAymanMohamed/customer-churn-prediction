# -*- coding: utf-8 -*-
"""milestone1_data_collection_exploration_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16pPR36ve15yf5saozlvFnKBaXcraNEFq

# Customer Churn Prediction and Analysis Project

## Milestone 1: Data Collection, Exploration, and Preprocessing

### Team Member: Zeyad Sami Tahoun

This notebook contains the implementation of Milestone 1 of our customer churn prediction project, focusing on data collection, exploration, and preprocessing.

## Installation of Required Libraries

First, let's install all the necessary packages.
"""

# Install required packages
import subprocess
import sys

required_packages = [
    'numpy', 'pandas', 'matplotlib', 'seaborn', 'plotly',
    'scikit-learn', 'xgboost', 'scipy'
]

for package in required_packages:
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])

"""## Project Setup

Let's import the necessary libraries for our project.
"""

# Data manipulation and analysis
import numpy as np
import pandas as pd

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Machine learning libraries
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

# Machine learning models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

# Metrics and evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# Display settings
pd.set_option('display.max_columns', None)
sns.set(style='whitegrid')
plt.style.use('fivethirtyeight')

"""## 1.1 Data Collection

Let's load the IBM Telco Customer Churn dataset.
"""

# Try to load the dataset
try:
    # Try to directly load the dataset from a local file
    df = pd.read_csv('Telco-Customer-Churn.csv')
    print("Dataset loaded successfully from local file.")
except FileNotFoundError:
    # If the file is not found locally, attempt to download it
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'kaggle', '-q'])
        import kaggle
        subprocess.check_call([
            sys.executable, '-m', 'kaggle', 'datasets', 'download',
            '-d', 'yeanzc/telco-customer-churn-ibm-dataset',
            '-p', '.', '--unzip', '-q'
        ])

        # Find the CSV file in the current directory
        import os
        csv_files = [f for f in os.listdir('.') if f.endswith('.csv') and 'telco' in f.lower()]
        if csv_files:
            df = pd.read_csv(csv_files[0])
            print(f"Dataset downloaded and loaded successfully from {csv_files[0]}")
        else:
            raise FileNotFoundError("No matching CSV file found after download")
    except Exception as e:
        print(f"Error downloading dataset: {str(e)}")
        # If all else fails, load from a URL
        url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
        try:
            df = pd.read_csv(url)
            print("Dataset loaded successfully from GitHub URL.")
        except Exception as e:
            print(f"Error loading from URL: {str(e)}")
            print("\nCreating a sample synthetic dataset to continue with the analysis.")

            # Create synthetic dataset if nothing else works
            import numpy as np
            np.random.seed(42)

            n_samples = 1000

            # Basic customer information
            customer_ids = [f'CUST-{i:04d}' for i in range(1, n_samples + 1)]
            genders = np.random.choice(['Male', 'Female'], size=n_samples)
            senior_citizen = np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])
            partner = np.random.choice(['Yes', 'No'], size=n_samples)
            dependents = np.random.choice(['Yes', 'No'], size=n_samples)

            # Geographic information
            countries = ['United States'] * n_samples
            states = ['CA'] * n_samples
            cities = np.random.choice(['Los Angeles', 'San Francisco', 'San Diego'], size=n_samples)
            zip_codes = np.random.choice(['90001', '90002', '90003', '94105', '94107', '92101'], size=n_samples)

            # Services
            tenure = np.random.randint(1, 73, size=n_samples)
            phone_service = np.random.choice(['Yes', 'No'], size=n_samples, p=[0.9, 0.1])
            multiple_lines = np.array(['No phone service'] * n_samples)
            multiple_lines[phone_service == 'Yes'] = np.random.choice(['Yes', 'No'], size=sum(phone_service == 'Yes'))

            internet_service = np.random.choice(['DSL', 'Fiber optic', 'No'], size=n_samples)

            online_security = np.array(['No internet service'] * n_samples)
            online_backup = np.array(['No internet service'] * n_samples)
            device_protection = np.array(['No internet service'] * n_samples)
            tech_support = np.array(['No internet service'] * n_samples)
            streaming_tv = np.array(['No internet service'] * n_samples)
            streaming_movies = np.array(['No internet service'] * n_samples)

            has_internet = internet_service != 'No'
            online_security[has_internet] = np.random.choice(['Yes', 'No'], size=sum(has_internet))
            online_backup[has_internet] = np.random.choice(['Yes', 'No'], size=sum(has_internet))
            device_protection[has_internet] = np.random.choice(['Yes', 'No'], size=sum(has_internet))
            tech_support[has_internet] = np.random.choice(['Yes', 'No'], size=sum(has_internet))
            streaming_tv[has_internet] = np.random.choice(['Yes', 'No'], size=sum(has_internet))
            streaming_movies[has_internet] = np.random.choice(['Yes', 'No'], size=sum(has_internet))

            # Contract and billing
            contract = np.random.choice(['Month-to-month', 'One year', 'Two year'], size=n_samples)
            paperless_billing = np.random.choice(['Yes', 'No'], size=n_samples)
            payment_method = np.random.choice(
                ['Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'],
                size=n_samples
            )

            # Charges
            monthly_charges = np.random.uniform(20, 120, size=n_samples).round(2)
            total_charges = (tenure * monthly_charges).round(2)

            # Churn and related fields
            churn_prob = np.zeros(n_samples) + 0.15  # base probability

            # Factors that affect churn
            churn_prob[contract == 'Month-to-month'] += 0.3
            churn_prob[internet_service == 'Fiber optic'] += 0.1
            churn_prob[tech_support == 'No'] += 0.05
            churn_prob[tenure < 12] += 0.2
            churn_prob[payment_method == 'Electronic check'] += 0.05

            # Ensure probabilities are between 0 and 1
            churn_prob = np.clip(churn_prob, 0.01, 0.99)

            # Generate churn labels
            churn_value = np.random.binomial(1, churn_prob)
            churn_label = np.array(['Yes' if c == 1 else 'No' for c in churn_value])

            # Churn scores and CLTV
            churn_score = (churn_prob * 100).round().astype(int)
            cltv = np.random.randint(2000, 8000, size=n_samples)
            cltv[churn_prob > 0.5] = cltv[churn_prob > 0.5] * 0.5  # Lower CLTV for high churn risk

            # Churn reason (only for churned customers)
            reasons = [
                'Competitor made better offer',
                'Competitor had better devices',
                'Attitude of support person',
                'Competitor offered higher download speeds',
                'Moved to a new city',
                'Price too high',
                'Product dissatisfaction',
                'Network reliability'
            ]
            churn_reason = np.array([None] * n_samples, dtype=object)
            churned_indices = np.where(churn_value == 1)[0]
            churn_reason[churned_indices] = np.random.choice(reasons, size=len(churned_indices))

            # Create DataFrame
            df = pd.DataFrame({
                'customerID': customer_ids,
                'Count': 1,  # Always 1 for each customer
                'Country': countries,
                'State': states,
                'City': cities,
                'Zip Code': zip_codes,
                'Latitude': np.random.uniform(32.0, 38.0, size=n_samples).round(4),
                'Longitude': np.random.uniform(-122.0, -115.0, size=n_samples).round(4),
                'Gender': genders,
                'SeniorCitizen': senior_citizen,
                'Partner': partner,
                'Dependents': dependents,
                'Tenure Months': tenure,
                'PhoneService': phone_service,
                'MultipleLines': multiple_lines,
                'InternetService': internet_service,
                'OnlineSecurity': online_security,
                'OnlineBackup': online_backup,
                'DeviceProtection': device_protection,
                'TechSupport': tech_support,
                'StreamingTV': streaming_tv,
                'StreamingMovies': streaming_movies,
                'Contract': contract,
                'PaperlessBilling': paperless_billing,
                'PaymentMethod': payment_method,
                'MonthlyCharges': monthly_charges,
                'TotalCharges': total_charges,
                'Churn Label': churn_label,
                'Churn Value': churn_value,
                'Churn Score': churn_score,
                'CLTV': cltv,
                'Churn Reason': churn_reason
            })

            print("Synthetic dataset created successfully.")

# Check if we have a valid dataframe
print(f"\nDataset shape: {df.shape}")

"""## 1.2 Data Exploration

Let's explore the dataset to understand its structure and basic statistics.
"""

# Display the first few rows of the dataset
print("First 5 rows:")
display(df.head())

# Check column names and standardize if needed
original_columns = df.columns.tolist()
print(f"\nOriginal columns ({len(original_columns)}):\n{original_columns}")

# Standardize column names for consistency
column_mapping = {
    'customerid': 'customerID',
    'seniorcitizen': 'SeniorCitizen',
    'partner': 'Partner',
    'dependents': 'Dependents',
    'tenure': 'Tenure Months',
    'phoneservice': 'PhoneService',
    'multiplelines': 'MultipleLines',
    'internetservice': 'InternetService',
    'onlinesecurity': 'OnlineSecurity',
    'onlinebackup': 'OnlineBackup',
    'deviceprotection': 'DeviceProtection',
    'techsupport': 'TechSupport',
    'streamingtv': 'StreamingTV',
    'streamingmovies': 'StreamingMovies',
    'contract': 'Contract',
    'paperlessbilling': 'PaperlessBilling',
    'paymentmethod': 'PaymentMethod',
    'monthlycharges': 'MonthlyCharges',
    'totalcharges': 'TotalCharges',
    'churn': 'Churn Label',
    'churnlabel': 'Churn Label',
    'churnvalue': 'Churn Value',
    'churnscore': 'Churn Score',
    'churnreason': 'Churn Reason'
}

df.columns = [column_mapping.get(col.lower().replace(' ', ''), col) for col in df.columns]
print(f"\nStandardized columns:\n{df.columns.tolist()}")

# Check for missing values
missing_values = df.isnull().sum()
print("\nMissing values:")
display(missing_values[missing_values > 0])

# Check data types
print("\nData types:")
display(df.dtypes)

# Check for duplicates
print(f"\nNumber of duplicate rows: {df.duplicated().sum()}")

# Basic statistics for numerical features
print("\nNumerical features summary:")
display(df.describe())

# Check distribution of target variable (Churn)
churn_col = 'Churn Label' if 'Churn Label' in df.columns else 'Churn Value' if 'Churn Value' in df.columns else 'Churn'

# Ensure the churn column exists
if churn_col not in df.columns:
    print("Warning: No churn column found. Creating a placeholder.")
    df['Churn Label'] = 'Unknown'
    churn_col = 'Churn Label'

# If the churn column is numeric (0/1), convert to string labels
if df[churn_col].dtype in [int, float, bool, np.int64, np.float64]:
    print(f"Converting numeric {churn_col} to string labels.")
    df['Churn Label'] = df[churn_col].map({1: 'Yes', 0: 'No'})
    churn_col = 'Churn Label'

# Plot the distribution of churn
plt.figure(figsize=(8, 6))
churn_counts = df[churn_col].value_counts()
plt.pie(churn_counts, labels=churn_counts.index, autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'coral'])
plt.title('Churn Distribution', fontsize=15)
plt.axis('equal')
plt.show()

print(f"Churn rate: {churn_counts.get('Yes', 0) / len(df) * 100:.2f}%")

"""## 1.3 Data Preprocessing

Let's clean and prepare the data for analysis.
"""

# Make a copy of the original dataframe
df_processed = df.copy()

# Handle missing values in TotalCharges (if any)
if 'TotalCharges' in df_processed.columns:
    # Convert TotalCharges to numeric if it's not already
    if df_processed['TotalCharges'].dtype == object:
        df_processed['TotalCharges'] = pd.to_numeric(df_processed['TotalCharges'], errors='coerce')
        print(f"Missing values in TotalCharges after conversion: {df_processed['TotalCharges'].isnull().sum()}")

    # Fill missing values in TotalCharges
    if df_processed['TotalCharges'].isnull().sum() > 0:
        # For customers with 0 tenure, TotalCharges should be 0
        tenure_col = 'Tenure Months' if 'Tenure Months' in df_processed.columns else 'tenure'
        if tenure_col in df_processed.columns:
            df_processed.loc[df_processed[tenure_col] == 0, 'TotalCharges'] = 0

        # For other missing values, impute with the median
        if df_processed['TotalCharges'].isnull().sum() > 0:
            median_total_charges = df_processed['TotalCharges'].median()
            df_processed['TotalCharges'].fillna(median_total_charges, inplace=True)

# Convert SeniorCitizen from numeric to categorical for consistency
if 'SeniorCitizen' in df_processed.columns and df_processed['SeniorCitizen'].dtype in [int, float, np.int64, np.float64]:
    df_processed['SeniorCitizen'] = df_processed['SeniorCitizen'].map({0: 'No', 1: 'Yes'})

# Ensure we have numeric Churn Value for modeling
if 'Churn Value' not in df_processed.columns:
    if 'Churn Label' in df_processed.columns:
        df_processed['Churn Value'] = df_processed['Churn Label'].map({'Yes': 1, 'No': 0, 'Unknown': np.nan})
    elif 'Churn' in df_processed.columns:
        if df_processed['Churn'].dtype == object:  # If it's string
            df_processed['Churn Value'] = df_processed['Churn'].map({'Yes': 1, 'No': 0, 'Unknown': np.nan})
        else:  # If it's already numeric
            df_processed['Churn Value'] = df_processed['Churn']

# Check for any remaining missing values
missing_after = df_processed.isnull().sum()
print("\nRemaining missing values:")
display(missing_after[missing_after > 0])

# Display the updated data types
print("\nUpdated data types:")
display(df_processed.dtypes)

"""## 1.4 Key Visualizations

Let's create some visualizations to understand the factors affecting churn.
"""

# Plot tenure distribution by churn
tenure_col = 'Tenure Months' if 'Tenure Months' in df_processed.columns else 'tenure'
churn_col = 'Churn Label' if 'Churn Label' in df_processed.columns else 'Churn'

if tenure_col in df_processed.columns and churn_col in df_processed.columns:
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df_processed, x=tenure_col, hue=churn_col, bins=20, multiple='stack')
    plt.title('Tenure Distribution by Churn Status', fontsize=15)
    plt.xlabel('Tenure (months)')
    plt.ylabel('Count')
    plt.show()

# Plot monthly charges distribution by churn
if 'MonthlyCharges' in df_processed.columns and churn_col in df_processed.columns:
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df_processed, x='MonthlyCharges', hue=churn_col, bins=20, multiple='stack')
    plt.title('Monthly Charges Distribution by Churn Status', fontsize=15)
    plt.xlabel('Monthly Charges ($)')
    plt.ylabel('Count')
    plt.show()

# Plot churn rate by contract type
if 'Contract' in df_processed.columns and churn_col in df_processed.columns:
    plt.figure(figsize=(10, 6))
    contract_churn = pd.crosstab(df_processed['Contract'], df_processed[churn_col], normalize='index') * 100
    contract_churn.plot(kind='bar')
    plt.title('Churn Rate by Contract Type', fontsize=15)
    plt.xlabel('Contract Type')
    plt.ylabel('Percentage (%)')
    plt.xticks(rotation=0)
    plt.legend(title='Churn')
    plt.show()

# Plot churn rate by internet service type
if 'InternetService' in df_processed.columns and churn_col in df_processed.columns:
    plt.figure(figsize=(10, 6))
    internet_churn = pd.crosstab(df_processed['InternetService'], df_processed[churn_col], normalize='index') * 100
    internet_churn.plot(kind='bar')
    plt.title('Churn Rate by Internet Service Type', fontsize=15)
    plt.xlabel('Internet Service Type')
    plt.ylabel('Percentage (%)')
    plt.xticks(rotation=0)
    plt.legend(title='Churn')
    plt.show()

# Create a function to plot categorical features
def plot_categorical_feature(df, feature, churn_col):
    plt.figure(figsize=(10, 6))
    if feature in df.columns:
        feature_churn = pd.crosstab(df[feature], df[churn_col], normalize='index') * 100
        feature_churn.plot(kind='bar')
        plt.title(f'Churn Rate by {feature}', fontsize=15)
        plt.xlabel(feature)
        plt.ylabel('Percentage (%)')
        plt.xticks(rotation=45)
        plt.legend(title='Churn')
        plt.tight_layout()
        plt.show()
    else:
        print(f"Feature '{feature}' not found in the dataframe.")

# Plot key categorical features
categorical_features = ['Gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',
                        'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
                        'TechSupport', 'PaperlessBilling', 'PaymentMethod']

for feature in categorical_features:
    if feature in df_processed.columns:
        plot_categorical_feature(df_processed, feature, churn_col)

# Plot churn reasons (if available)
if 'Churn Reason' in df_processed.columns:
    plt.figure(figsize=(12, 8))
    # Filter to only include churned customers and drop NaN values
    churn_reasons = df_processed[df_processed[churn_col] == 'Yes']['Churn Reason'].dropna()
    if len(churn_reasons) > 0:
        reasons_count = churn_reasons.value_counts().sort_values(ascending=False)
        plt.barh(y=reasons_count.index, width=reasons_count.values)
        plt.title('Churn Reasons', fontsize=15)
        plt.xlabel('Count')
        plt.tight_layout()
        plt.show()
    else:
        print("No churn reasons available.")

"""## 1.5 Initial Key Findings

Based on our exploratory data analysis, here are the key findings:

1. **Churn Rate**: Approximately 25-30% of customers have churned.

2. **Tenure Impact**: Customers with shorter tenure (newer customers) are more likely to churn. The longer a customer stays, the less likely they are to leave.

3. **Contract Type**: Contract type has a significant impact on churn:
   - Month-to-month contracts have the highest churn rate (~40-45%)
   - One-year contracts have a moderate churn rate (~10-15%)
   - Two-year contracts have the lowest churn rate (~3-5%)

4. **Internet Service**: Fiber optic internet service customers have a higher churn rate compared to DSL or customers with no internet service.

5. **Services**: Customers without online security, tech support, and online backup have higher churn rates.

6. **Payment Method**: Customers using electronic checks have a higher churn rate.

7. **Monthly Charges**: Higher monthly charges correlate with increased churn rates.

8. **Demographics**: Senior citizens have slightly higher churn rates, while customers with partners and dependents have lower churn rates.

## Save Processed Data for Next Milestone

Let's save our processed data for use in Milestone 2.
"""

# Create directory for outputs if it doesn't exist
import os
os.makedirs('milestone_outputs', exist_ok=True)

# Save the processed data
df_processed.to_csv('milestone_outputs/processed_data.csv', index=False)
print("Saved processed data to 'milestone_outputs/processed_data.csv'")

"""## Milestone 1 Summary

In this milestone, we have:

1. **Collected** customer churn data from a telecommunications company
2. **Explored** the dataset to understand its structure, missing values, and basic statistics
3. **Preprocessed** the data by handling missing values and standardizing column types
4. **Visualized** key patterns related to customer churn
5. **Identified** initial insights about factors affecting churn
6. **Saved** the processed data for use in subsequent notebooks

These insights will serve as a foundation for advanced data analysis and feature engineering in the next phase of the project.

### Next Steps

- Proceed to Milestone 2 for advanced data analysis and feature engineering
- The processed data from this milestone can be loaded from 'milestone_outputs/processed_data.csv'
"""