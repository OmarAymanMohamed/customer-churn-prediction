# -*- coding: utf-8 -*-
"""milestone2_advanced_data_analysis_and_feature_engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MMzAVnJVNdsXTkFPOaz_bYEjgcTfkL5J

# Customer Churn Prediction and Analysis Project

## Milestone 2: Advanced Data Analysis and Feature Engineering

### Team Member: Hager Essa

This notebook contains the implementation of Milestone 2 of our customer churn prediction project, focusing on advanced data analysis and feature engineering.

## Installation of Required Libraries

First, let's install all the necessary packages.
"""

# Install required packages
import subprocess
import sys

required_packages = [
    'numpy', 'pandas', 'matplotlib', 'seaborn', 'plotly',
    'scikit-learn', 'xgboost', 'scipy'
]

for package in required_packages:
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])

"""## Project Setup

Let's import the necessary libraries for our project.
"""

# Data manipulation and analysis
import numpy as np
import pandas as pd

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Machine learning libraries
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif

# Machine learning models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

# Metrics and evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# Display settings
pd.set_option('display.max_columns', None)
sns.set(style='whitegrid')
plt.style.use('fivethirtyeight')

"""## Load and Prepare Data

For Milestone 2, we'll start with the preprocessed data from Milestone 1. Let's check if we have the saved processed data from Milestone 1; otherwise, we'll load and preprocess the raw data.
"""

# Attempt to load processed data from Milestone 1
import os

try:
    # Try to load the processed dataframe from Milestone 1
    df_processed = pd.read_csv('milestone_outputs/processed_data.csv')
    print("Successfully loaded processed data from Milestone 1.")

    # Define churn column for visualizations
    churn_col = 'Churn Label' if 'Churn Label' in df_processed.columns else 'Churn'

    print(f"Processed data shape: {df_processed.shape}")

except FileNotFoundError:
    print("Processed data from Milestone 1 not found. Loading and preprocessing raw data...")

    # If the processed data isn't available, load and process the raw data
    try:
        # Try to directly load the dataset from a local file
        df = pd.read_csv('Telco-Customer-Churn.csv')
        print("Dataset loaded successfully from local file.")
    except FileNotFoundError:
        # If the file is not found locally, attempt to download it
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'kaggle', '-q'])
            import kaggle
            subprocess.check_call([
                sys.executable, '-m', 'kaggle', 'datasets', 'download',
                '-d', 'yeanzc/telco-customer-churn-ibm-dataset',
                '-p', '.', '--unzip', '-q'
            ])

            # Find the CSV file in the current directory
            csv_files = [f for f in os.listdir('.') if f.endswith('.csv') and 'telco' in f.lower()]
            if csv_files:
                df = pd.read_csv(csv_files[0])
                print(f"Dataset downloaded and loaded successfully from {csv_files[0]}")
            else:
                raise FileNotFoundError("No matching CSV file found after download")
        except Exception as e:
            print(f"Error downloading dataset: {str(e)}")
            # If all else fails, load from a URL
            url = "https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv"
            try:
                df = pd.read_csv(url)
                print("Dataset loaded successfully from GitHub URL.")
            except Exception as e:
                print(f"Error loading from URL: {str(e)}")
                print("\nCreating a sample synthetic dataset to continue with the analysis.")

                # Create synthetic dataset if nothing else works
                # (Code for synthetic data generation omitted for brevity - refer to Milestone 1)
                # ...

    # Perform preprocessing steps from Milestone 1
    print("Preprocessing the data...")

    # Standardize column names for consistency
    column_mapping = {
        'customerid': 'customerID',
        'seniorcitizen': 'SeniorCitizen',
        'partner': 'Partner',
        'dependents': 'Dependents',
        'tenure': 'Tenure Months',
        'phoneservice': 'PhoneService',
        'multiplelines': 'MultipleLines',
        'internetservice': 'InternetService',
        'onlinesecurity': 'OnlineSecurity',
        'onlinebackup': 'OnlineBackup',
        'deviceprotection': 'DeviceProtection',
        'techsupport': 'TechSupport',
        'streamingtv': 'StreamingTV',
        'streamingmovies': 'StreamingMovies',
        'contract': 'Contract',
        'paperlessbilling': 'PaperlessBilling',
        'paymentmethod': 'PaymentMethod',
        'monthlycharges': 'MonthlyCharges',
        'totalcharges': 'TotalCharges',
        'churn': 'Churn Label',
        'churnlabel': 'Churn Label',
        'churnvalue': 'Churn Value',
        'churnscore': 'Churn Score',
        'churnreason': 'Churn Reason'
    }

    df.columns = [column_mapping.get(col.lower().replace(' ', ''), col) for col in df.columns]

    # Make a copy of the original dataframe
    df_processed = df.copy()

    # Handle missing values in TotalCharges (if any)
    if 'TotalCharges' in df_processed.columns:
        # Convert TotalCharges to numeric if it's not already
        if df_processed['TotalCharges'].dtype == object:
            df_processed['TotalCharges'] = pd.to_numeric(df_processed['TotalCharges'], errors='coerce')

        # Fill missing values in TotalCharges
        if df_processed['TotalCharges'].isnull().sum() > 0:
            # For customers with 0 tenure, TotalCharges should be 0
            tenure_col = 'Tenure Months' if 'Tenure Months' in df_processed.columns else 'tenure'
            if tenure_col in df_processed.columns:
                df_processed.loc[df_processed[tenure_col] == 0, 'TotalCharges'] = 0

            # For other missing values, impute with the median
            if df_processed['TotalCharges'].isnull().sum() > 0:
                median_total_charges = df_processed['TotalCharges'].median()
                df_processed['TotalCharges'].fillna(median_total_charges, inplace=True)

    # Convert SeniorCitizen from numeric to categorical for consistency
    if 'SeniorCitizen' in df_processed.columns and df_processed['SeniorCitizen'].dtype in [int, float, np.int64, np.float64]:
        df_processed['SeniorCitizen'] = df_processed['SeniorCitizen'].map({0: 'No', 1: 'Yes'})

    # Ensure we have numeric Churn Value for modeling
    if 'Churn Value' not in df_processed.columns:
        if 'Churn Label' in df_processed.columns:
            df_processed['Churn Value'] = df_processed['Churn Label'].map({'Yes': 1, 'No': 0, 'Unknown': np.nan})
        elif 'Churn' in df_processed.columns:
            if df_processed['Churn'].dtype == object:  # If it's string
                df_processed['Churn Value'] = df_processed['Churn'].map({'Yes': 1, 'No': 0, 'Unknown': np.nan})
            else:  # If it's already numeric
                df_processed['Churn Value'] = df_processed['Churn']

    # Define churn column for visualizations
    churn_col = 'Churn Label' if 'Churn Label' in df_processed.columns else 'Churn'

    # Create directory for outputs if running independently
    os.makedirs('milestone_outputs', exist_ok=True)

    # Save the processed data for reference
    df_processed.to_csv('milestone_outputs/processed_data.csv', index=False)
    print("Saved processed data to 'milestone_outputs/processed_data.csv'")

print(f"\nDataset ready for feature engineering.")

"""## 2.1 Feature Engineering

Let's create new features to improve our model's performance.
"""

# Create a copy of the processed dataframe for feature engineering
df_fe = df_processed.copy()

# Identify the tenure column
tenure_col = 'Tenure Months' if 'Tenure Months' in df_fe.columns else 'tenure'

# 1. Tenure groups
if tenure_col in df_fe.columns:
    bins = [0, 12, 24, 36, 48, 60, 72]
    labels = ['0-12', '13-24', '25-36', '37-48', '49-60', '60+']
    df_fe['Tenure_Group'] = pd.cut(df_fe[tenure_col], bins=bins, labels=labels, right=False)

# 2. Number of services subscribed
service_columns = [col for col in ['PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',
                                  'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']
                  if col in df_fe.columns]

if service_columns:
    df_fe['Service_Count'] = df_fe[service_columns].apply(lambda row: row.str.contains('Yes').sum(), axis=1)

# 3. Has tech services
tech_columns = [col for col in ['OnlineSecurity', 'OnlineBackup', 'TechSupport'] if col in df_fe.columns]
if tech_columns:
    df_fe['Has_Tech_Service'] = df_fe[tech_columns].apply(lambda row: 'Yes' if 'Yes' in row.values else 'No', axis=1)

# 4. Has streaming services
stream_columns = [col for col in ['StreamingTV', 'StreamingMovies'] if col in df_fe.columns]
if stream_columns:
    df_fe['Has_Streaming_Service'] = df_fe[stream_columns].apply(lambda row: 'Yes' if 'Yes' in row.values else 'No', axis=1)

# 5. Is new customer
if tenure_col in df_fe.columns:
    df_fe['Is_New_Customer'] = df_fe[tenure_col] <= 6
    df_fe['Is_New_Customer'] = df_fe['Is_New_Customer'].map({True: 'Yes', False: 'No'})

# 6. Has family
if 'Partner' in df_fe.columns and 'Dependents' in df_fe.columns:
    df_fe['Has_Family'] = ((df_fe['Partner'] == 'Yes') | (df_fe['Dependents'] == 'Yes')).map({True: 'Yes', False: 'No'})

# 7. Monthly charges category
if 'MonthlyCharges' in df_fe.columns:
    bins = [0, 35, 70, 120]
    labels = ['Low', 'Medium', 'High']
    df_fe['Monthly_Charges_Category'] = pd.cut(df_fe['MonthlyCharges'], bins=bins, labels=labels, right=False)

# 8. Customer segments based on tenure and monthly charges
if tenure_col in df_fe.columns and 'MonthlyCharges' in df_fe.columns:
    tenure_cut = 12  # 1 year
    charge_cut = df_fe['MonthlyCharges'].median()

    conditions = [
        (df_fe[tenure_col] <= tenure_cut) & (df_fe['MonthlyCharges'] <= charge_cut),
        (df_fe[tenure_col] <= tenure_cut) & (df_fe['MonthlyCharges'] > charge_cut),
        (df_fe[tenure_col] > tenure_cut) & (df_fe['MonthlyCharges'] <= charge_cut),
        (df_fe[tenure_col] > tenure_cut) & (df_fe['MonthlyCharges'] > charge_cut)
    ]

    segment_names = ['New Low-Value', 'New High-Value', 'Established Low-Value', 'Established High-Value']
    df_fe['Customer_Segment'] = np.select(conditions, segment_names, default='Other')

# Display the new features
print("New features added to the dataset:")
new_features = [col for col in ['Tenure_Group', 'Service_Count', 'Has_Tech_Service', 'Has_Streaming_Service',
                               'Is_New_Customer', 'Has_Family', 'Monthly_Charges_Category', 'Customer_Segment']
               if col in df_fe.columns]
display(df_fe[new_features].head())

"""## 2.2 Visualizing Engineered Features

Let's visualize the engineered features to understand their relationship with churn.
"""

# Define churn column
churn_col = 'Churn Label' if 'Churn Label' in df_fe.columns else 'Churn'

# Visualize churn rate by customer segment
if 'Customer_Segment' in df_fe.columns:
    plt.figure(figsize=(10, 6))
    segment_churn = pd.crosstab(df_fe['Customer_Segment'], df_fe[churn_col], normalize='index') * 100
    segment_churn.plot(kind='bar')
    plt.title('Churn Rate by Customer Segment', fontsize=15)
    plt.xlabel('Customer Segment')
    plt.ylabel('Percentage (%)')
    plt.xticks(rotation=45)
    plt.legend(title='Churn')
    plt.tight_layout()
    plt.show()

# Visualize churn rate by service count
if 'Service_Count' in df_fe.columns:
    plt.figure(figsize=(10, 6))
    service_count_churn = pd.crosstab(df_fe['Service_Count'], df_fe[churn_col], normalize='index') * 100
    service_count_churn.plot(kind='bar')
    plt.title('Churn Rate by Number of Services', fontsize=15)
    plt.xlabel('Number of Services')
    plt.ylabel('Percentage (%)')
    plt.xticks(rotation=0)
    plt.legend(title='Churn')
    plt.show()

# Visualize churn rate by tech service
if 'Has_Tech_Service' in df_fe.columns:
    plt.figure(figsize=(10, 6))
    tech_service_churn = pd.crosstab(df_fe['Has_Tech_Service'], df_fe[churn_col], normalize='index') * 100
    tech_service_churn.plot(kind='bar')
    plt.title('Churn Rate by Tech Service Subscription', fontsize=15)
    plt.xlabel('Has Tech Service')
    plt.ylabel('Percentage (%)')
    plt.xticks(rotation=0)
    plt.legend(title='Churn')
    plt.show()

# Create an interactive dashboard using Plotly
from plotly.subplots import make_subplots

if 'Contract' in df_fe.columns and 'Tenure_Group' in df_fe.columns and 'Customer_Segment' in df_fe.columns:
    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Churn Rate by Contract Type', 'Churn Rate by Tenure Group',
                       'Churn Rate by Customer Segment', 'Churn Rate by Service Count'),
        specs=[[{'type': 'bar'}, {'type': 'bar'}],
               [{'type': 'bar'}, {'type': 'bar'}]]
    )

    # 1. Churn rate by contract type
    contract_churn = pd.crosstab(df_fe['Contract'], df_fe[churn_col], normalize='index') * 100
    fig.add_trace(
        go.Bar(x=contract_churn.index, y=contract_churn.get('Yes', 0), name='Churn Rate'),
        row=1, col=1
    )

    # 2. Churn rate by tenure group
    tenure_group_churn = pd.crosstab(df_fe['Tenure_Group'], df_fe[churn_col], normalize='index') * 100
    fig.add_trace(
        go.Bar(x=tenure_group_churn.index, y=tenure_group_churn.get('Yes', 0), name='Churn Rate'),
        row=1, col=2
    )

    # 3. Churn rate by customer segment
    segment_churn = pd.crosstab(df_fe['Customer_Segment'], df_fe[churn_col], normalize='index') * 100
    fig.add_trace(
        go.Bar(x=segment_churn.index, y=segment_churn.get('Yes', 0), name='Churn Rate'),
        row=2, col=1
    )

    # 4. Churn rate by service count
    if 'Service_Count' in df_fe.columns:
        service_count_churn = pd.crosstab(df_fe['Service_Count'], df_fe[churn_col], normalize='index') * 100
        fig.add_trace(
            go.Bar(x=service_count_churn.index.astype(str), y=service_count_churn.get('Yes', 0), name='Churn Rate'),
            row=2, col=2
        )

    # Update layout
    fig.update_layout(
        title_text='Customer Churn Analysis Dashboard',
        height=800,
        width=1000,
        showlegend=False
    )

    # Update y-axes
    fig.update_yaxes(title_text='Churn Rate (%)', range=[0, 100])

    # Show dashboard
    fig.show()

"""## 2.3 Statistical Analysis of Features

Let's perform some statistical tests to understand the relationship between features and churn.
"""

# Chi-square test for categorical variables
from scipy.stats import chi2_contingency

# Function to perform chi-square test
def chi_square_test(df, feature, target):
    if feature in df.columns and target in df.columns:
        contingency_table = pd.crosstab(df[feature], df[target])
        chi2, p, dof, expected = chi2_contingency(contingency_table)
        return {
            'Feature': feature,
            'Chi-Square': chi2,
            'p-value': p,
            'Significant': 'Yes' if p < 0.05 else 'No'
        }
    return None

# Categorical features to test
categorical_features = [
    'Gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
    'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',
    'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',
    'Tenure_Group', 'Has_Tech_Service', 'Has_Streaming_Service', 'Is_New_Customer',
    'Has_Family', 'Monthly_Charges_Category', 'Customer_Segment'
]

# Perform chi-square test for each categorical feature
chi_square_results = []
for feature in categorical_features:
    result = chi_square_test(df_fe, feature, churn_col)
    if result:
        chi_square_results.append(result)

# Create DataFrame with results
chi_square_df = pd.DataFrame(chi_square_results)
chi_square_df = chi_square_df.sort_values(by='Chi-Square', ascending=False)

# Display results
print("Chi-Square Test Results for Categorical Features:")
display(chi_square_df)

# T-test for numerical variables
from scipy.stats import ttest_ind

# Function to perform t-test
def t_test(df, feature, target):
    if feature in df.columns and target in df.columns:
        # Get groups
        group1 = df[df[target] == 'Yes'][feature]
        group2 = df[df[target] == 'No'][feature]

        # Perform t-test
        t_stat, p_value = ttest_ind(group1, group2, equal_var=False)

        # Calculate mean difference
        mean_diff = group1.mean() - group2.mean()

        return {
            'Feature': feature,
            'T-Statistic': t_stat,
            'p-value': p_value,
            'Mean Difference': mean_diff,
            'Significant': 'Yes' if p_value < 0.05 else 'No'
        }
    return None

# Numerical features to test
numerical_features = ['Tenure Months', 'MonthlyCharges', 'TotalCharges', 'Service_Count']

# Perform t-test for each numerical feature
t_test_results = []
for feature in numerical_features:
    if feature in df_fe.columns:
        result = t_test(df_fe, feature, churn_col)
        if result:
            t_test_results.append(result)

# Create DataFrame with results
t_test_df = pd.DataFrame(t_test_results)
t_test_df = t_test_df.sort_values(by='T-Statistic', ascending=False)

# Display results
print("T-Test Results for Numerical Features:")
display(t_test_df)

# Correlation between numerical features and churn
# Convert churn to numeric for correlation
df_fe_corr = df_fe.copy()
df_fe_corr['Churn_Numeric'] = df_fe_corr[churn_col].map({'Yes': 1, 'No': 0})

# Select numerical features
numerical_cols = ['Tenure Months', 'MonthlyCharges', 'TotalCharges', 'Service_Count', 'Churn_Numeric']
numerical_cols = [col for col in numerical_cols if col in df_fe_corr.columns]

# Compute correlation matrix
correlation_matrix = df_fe_corr[numerical_cols].corr()

# Plot correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix of Numerical Features with Churn', fontsize=15)
plt.tight_layout()
plt.show()

"""## 2.4 Advanced Feature Selection

Let's identify the most important features for our model using statistical feature selection methods.
"""

# Prepare data for feature selection
# Get a copy of the feature-engineered dataset
feature_selection_df = df_fe.copy()

# Define target variable
y = feature_selection_df['Churn Value'] if 'Churn Value' in feature_selection_df.columns else feature_selection_df[churn_col].map({'Yes': 1, 'No': 0})

# Remove non-feature columns
drop_cols = ['customerID', 'Churn Label', 'Churn Value', 'Churn Score', 'CLTV', 'Churn Reason',
             'Count', 'Country', 'State', 'City', 'Zip Code', 'Latitude', 'Longitude']
drop_cols = [col for col in drop_cols if col in feature_selection_df.columns]
X = feature_selection_df.drop(drop_cols, axis=1)

# Encode categorical variables
categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Feature selection using ANOVA F-value
print("Feature Selection using ANOVA F-value:")
f_selector = SelectKBest(f_classif, k=15)  # Select top 15 features
f_selector.fit(X_encoded, y)

# Get feature scores
f_scores = pd.DataFrame({
    'Feature': X_encoded.columns,
    'F-Score': f_selector.scores_,
    'p-value': f_selector.pvalues_
})
f_scores = f_scores.sort_values('F-Score', ascending=False)

# Display top features
print("Top 15 Features by F-Score:")
display(f_scores.head(15))

# Plot feature importance
plt.figure(figsize=(12, 8))
sns.barplot(x='F-Score', y='Feature', data=f_scores.head(15))
plt.title('Feature Importance by F-Score', fontsize=15)
plt.tight_layout()
plt.show()

# Feature selection using Mutual Information
print("Feature Selection using Mutual Information:")
mi_selector = SelectKBest(mutual_info_classif, k=15)  # Select top 15 features
mi_selector.fit(X_encoded, y)

# Get feature scores
mi_scores = pd.DataFrame({
    'Feature': X_encoded.columns,
    'MI-Score': mi_selector.scores_
})
mi_scores = mi_scores.sort_values('MI-Score', ascending=False)

# Display top features
print("Top 15 Features by Mutual Information:")
display(mi_scores.head(15))

# Plot feature importance
plt.figure(figsize=(12, 8))
sns.barplot(x='MI-Score', y='Feature', data=mi_scores.head(15))
plt.title('Feature Importance by Mutual Information', fontsize=15)
plt.tight_layout()
plt.show()

"""## Save Feature-Engineered Data for Next Milestone

Let's save our feature-engineered dataset for use in Milestone 3.
"""

# Save the feature-engineered dataframe to a CSV file
os.makedirs('milestone_outputs', exist_ok=True)
df_fe.to_csv('milestone_outputs/feature_engineered_data.csv', index=False)
print("Saved feature-engineered data to 'milestone_outputs/feature_engineered_data.csv'")

"""## Milestone 2 Summary

In this milestone, we have:

1. **Created** several engineered features to improve model performance
2. **Visualized** the relationship between these features and churn
3. **Performed** statistical tests to quantify the significance of each feature
4. **Identified** the most important features using different feature selection techniques
5. **Built** an interactive dashboard to explore churn patterns
6. **Saved** the feature-engineered data for use in subsequent notebooks

The engineered features and insights from this milestone lay the groundwork for developing effective machine learning models in the next phase of the project.

### Next Steps

- Proceed to Milestone 3 for machine learning model development and optimization
- The feature-engineered data from this milestone can be loaded from 'milestone_outputs/feature_engineered_data.csv'
"""